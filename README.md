**For the Custom Res-SE Net scripts, see this repository [Custom Res-SE Net](https://github.com/nedatghd/Lung-Cancer-Diagnosis-from-2D-18F---PET-CT-Images/tree/main).**

Custom Res-SE Net is a custom-designed convolutional neural network for image classification, featuring a blend of Residual Blocks with Squeeze-and-Excitation (SE) attention and standard convolutional blocks. The architecture alternates between these two block types and culminates in a fully connected classifier.
<img width="895" height="462" alt="image" src="https://github.com/user-attachments/assets/4ac8c86c-759e-45a5-b3f4-710169742189" />
<img width="833" height="544" alt="image" src="https://github.com/user-attachments/assets/fff5322f-0d6e-440c-afe0-7bc221a0b2c9" />

 Schematic illustration of two primary building blocks: a) Res-SE Block, which integrates residual connections with channel-wise attention (Squeeze-and-Excitation), b) Norm. Conv Block consists of standard convolutional operations without skip connections 


________________________________________
ğŸ§± Architecture Summary

Layer	Block Type	In Channels	Out Channels	Stride	SE Attention	MaxPooling	Dropout

1	ResidualSEBlock	3	16	2	âœ…	2Ã—2	0.3

2	NormalConvBlock	16	32	1	âŒ	2Ã—2	0.3

3	ResidualSEBlock	32	64	1	âœ…	2Ã—2	0.3

4	NormalConvBlock	64	128	1	âŒ	2Ã—2	0.4

5	ResidualSEBlock	128	256	1	âœ…	2Ã—2	0.4

6	NormalConvBlock	256	512	1	âŒ	2Ã—2	0.4

7	GlobalAvgPool	â€”	â€”	â€”	â€”	â€”	â€”

8	Fully Connected	512	256 â†’ N/A	â€”	â€”	â€”	0.6
________________________________________
ğŸ§  Block Definitions

1. ResidualSEBlock
   
â€¢	A residual convolutional block with Squeeze-and-Excitation attention.

â€¢	Structure:

o	Two 3Ã—3 convolutions with BatchNorm and LeakyReLU (except last activation).

o	SE attention via global average pooling and channel-wise gating.

o	Identity or 1Ã—1 convolution-based skip connection if shape mismatch.

o	Final output passed through LeakyReLU.

2. NormalConvBlock
   
â€¢	A basic convolutional block without residual connection.

â€¢	Structure:

o	Two 3Ã—3 convolutions, both followed by BatchNorm and LeakyReLU.
________________________________________
ğŸ§® Classifier Head

After all convolutional layers:

â€¢	AdaptiveAvgPool2d(1) compresses spatial dimensions to 1Ã—1.

â€¢	Output is flattened to shape [batch_size, 512].

â€¢	Fully connected layers:

o	Linear(512 â†’ 256) â†’ LeakyReLU â†’ Dropout(0.6)

o	Linear(256 â†’ num_classes)
________________________________________
ğŸ” Key Features

â€¢	Squeeze-and-Excitation (SE): Channel-wise attention enhances feature representation.

â€¢	Adaptive Pooling: Makes the model flexible to input image size (post-convolution).

â€¢	LeakyReLU Activations: Avoids dying ReLU problem, improving feature flow.

â€¢	Heavy Regularization:

o	Dropout2D after each layer (0.3â€“0.4)

o	Dropout(0.6) before final classification
________________________________________
ğŸ“„ Data Preparation and Loading Pipeline

ğŸ”§ Hyperparameter Setup

The script defines key hyperparameters that control training dynamics and regularization:

â€¢	Epochs: 200 â€” the number of times the model will iterate through the training data.

â€¢	Batch Size: 64 â€” determines the number of samples processed before the model updates.

â€¢	Learning Rate: 0.001 â€” controls the step size in the weight update.

â€¢	Weight Decay: 1e-4 â€” a form of L2 regularization to reduce overfitting.

â€¢	Early Stopping Patience: 20 â€” training halts if no improvement is seen over this many epochs.
________________________________________
ğŸ‹ï¸ Training Transformations (Data Augmentation)

A robust set of augmentations is applied to artificially expand the dataset and improve generalization:

â€¢	Resize((245, 457)): Standardizes input image dimensions.

â€¢	RandomHorizontalFlip(p=0.5): Horizontally flips images with 50% probability.

â€¢	RandomVerticalFlip(p=0.3): Vertically flips images with 30% probability.

â€¢	RandomRotation(25): Rotates images randomly within Â±25 degrees.

â€¢	ColorJitter(...): Randomly changes brightness, contrast, and saturation.

â€¢	RandomAffine(...): Applies affine transformation with shearing and scaling.

â€¢	ToTensor(): Converts images into PyTorch tensors.

ğŸ” Testing/Validation Transformations

A minimal set of transformations is applied to ensure consistency and performance evaluation:

â€¢	Resize((245, 457)): Matches the training input size.

â€¢	ToTensor(): Converts images to tensor format.
